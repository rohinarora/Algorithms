
<!-- saved from url=(0043)http://people.csail.mit.edu/moitra/408.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252"><title>18.408  Algorithmic Aspects of Machine Learning, Fall 2017 </title></head><body background="./18.408 Algorithmic Aspects of Machine Learning, Fall 2017_files/a.html">
<font face="georgia">
<center>
<h1>18.408: Algorithmic Aspects of Machine Learning</h1>
<h3>Fall 2017</h3>
<hr width="60%">
</center>



<br><br>Modern machine learning systems are often built on top of algorithms that do not have provable guarantees, and it is the subject of debate when and why they work. In this class, we will focus on designing algorithms whose performance we can rigorously analyze for fundamental machine learning problems. We will cover topics such as: nonnegative matrix factorization, tensor decomposition, sparse coding, learning mixture models, matrix completion and inference in graphical models. Almost all of these problems are computationally hard in the worst-case and so developing an algorithmic theory is about  (1) choosing the right models in which to study these problems and (2) developing the appropriate mathematical tools (often from probability, geometry or algebra) in order to rigorously analyze existing heuristics, or to design fundamentally new algorithms. 

<br><br><b><font color="#800000">Announcement:</font></b> Course evaluations are
<a href="http://web.mit.edu/subjectevaluation/">here</a>, please take a few minutes to fill it out before it closes on Monday, December 18th at 9am!

<br><br><b><font color="#800000">Announcement:</font></b> The final project description is posted 
<a href="http://people.csail.mit.edu/moitra/docs/projects408.pdf">here</a> and due on December 13th, by email

<a name="staff"></a>
<h2>Course Information</h2>

<ul>

<li><b>Instructor: <a href="http://people.csail.mit.edu/moitra/">Ankur Moitra</a></b>
</li>
<br>



<li><b>Lectures:</b> Monday and Wednesday 1:00-2:30, <b><font color="#800000">room change (!)</font></b> 4-237
</li>
<br>

<li><b>Teaching Assistant: <a href="http://math.mit.edu/~awein/">Alex Wein</a></b>
</li>
<br>

<li><b>Prerequisites:</b> A course in algorithms (6.046/18.410 or equivalent) and probability (6.041/18.440 or equivalent). You will need a strong background in algorithms, probability and linear algebra.  
</li>
<br>

<li><b>Textbook:</b> We will use this <b><a href="http://people.csail.mit.edu/moitra/docs/bookex.pdf"><font color="#800000">monograph</font></a></b>. Lecture notes and/or presentations covering new topics will be provided.
</li> 
<br>

<li><b>Office Hours: </b> Monday and Wednesday 2:30-3:30 (meet after class)
</li> 
<br>

<li><b>Assessment:</b> Students will be expected to solve a handful of problem sets, and complete a research-oriented final project. This could be either a survey, or original research; students will be encouraged to find connections between the course material and their own research interests.
</li></ul>

<h2>Problem Sets</h2>

<ul>

<li> Problem Set 1: <a href="http://people.csail.mit.edu/moitra/docs/18408ps1.pdf">[pdf]</a>
</li>


<li> Problem Set 2: <a href="http://people.csail.mit.edu/moitra/docs/18408ps2.pdf">[pdf]</a>
</li>


<li> Problem Set 3: <a href="http://people.csail.mit.edu/moitra/docs/18408ps3.pdf">[pdf]</a>
</li>


<li> Final Project: <a href="http://people.csail.mit.edu/moitra/docs/projects408.pdf">[pdf]</a>
</li>

</ul>

<h2>Additional Notes</h2>

<ul>

<li> Alex Wein Guest Lecture: Message Passing and State Evolution <a href="http://people.csail.mit.edu/moitra/docs/state_evolution.pdf">[pdf]</a>
</li>


<li> Lecture 21: Matrix Completion and Rademacher Complexity <a href="http://people.csail.mit.edu/moitra/docs/rademachernotes.pdf">[pdf]</a>
</li>


<li> Lecture 22: More Matrix Completion <a href="http://people.csail.mit.edu/moitra/docs/rademachernotes2.pdf">[pdf]</a>
</li>


<li> Lecture 23: Nonconvex Optimization and the Strict Saddle Property <a href="http://people.csail.mit.edu/moitra/docs/strictsaddlenotes.pdf">[pdf]</a>
</li>

<li> Lecture 24: No Spurious Local Minima <a href="http://people.csail.mit.edu/moitra/docs/nospuriousnotes.pdf">[pdf]</a>
</li>

</ul>

<a name="outline"></a>
<h2>Course Outline</h2>


Here is a tentative outline for the course:

<ul>

 <b> Nonnegative Matrix Factorization</b> <a href="http://people.csail.mit.edu/moitra/docs/Provable8.pdf">[slides]</a>
 <br>
<ul>
<li>Qualitative Comparisons to SVD</li>
<li>New Algorithms via Separability</li>
<li>Applications to Topic Models</li>
</ul>
<br>

<ul>
D. Lee and S. Seung. <a href="http://www.nature.com/nature/journal/v401/n6755/full/401788a0.html">Learning the Parts of Objects by Nonnegative Matrix Factorization</a>, Nature 1999.
<br>
S. Vavasis. <a href="http://arxiv.org/abs/0708.4149">On the Complexity of Nonnegative Matrix Factorization</a>, SIOPT 2009.
<br>
S. Arora, R. Ge, R. Kannan and A. Moitra. <a href="http://arxiv.org/abs/1111.0952">Computing a Nonnegative Matrix Factorization -- Provably</a>, STOC 2012.
<br>
S. Arora, R. Ge and A. Moitra. <a href="http://arxiv.org/abs/1204.1956">Learning Topic Models -- Going Beyond SVD</a>, FOCS 2012.
<br>
S. Arora et al. <a href="http://arxiv.org/abs/1212.4777">A Practical Algorithm for Topic Modeling with Provable Guarantees</a>, ICML 2013.
<br>
</ul>
<br>


 <b> Discussion: </b> When does well-posedness lead to better algorithms?
 <ul>
 <br>
M. Balcan, A. Blum and A. Gupta. <a href="http://www.cs.cmu.edu/~ninamf/papers/clustering-bbg-jacm.pdf">Clustering under Approximation Stability</a>, JACM 2013.
</ul>
<br>

<b> Tensor Decompositions and Applications</b> <a href="http://people.csail.mit.edu/moitra/docs/Tensors.pdf">[slides]</a>
<br>
<ul>
<li>Tensor Rank, Border Rank and the Rotation Problem</li>
<li>Jennrich's Algorithm and the Generalized Eigenvalue Problem</li>
<li>Learning HMMs</li>
<li>Mixed Membership Models and Community Detection</li>
</ul>
<br>

<ul>
C. Hillar and L. Lim. <a href="http://arxiv.org/abs/0911.1393">Most Tensor Problems are NP-hard</a>, JACM 2013.
<br>
E. Mossel and S. Roch. <a href="http://arxiv.org/abs/cs.LG/0502076">Learning Nonsingular Phylogenies and Hidden Markov Models</a>, STOC 2005.
<br>
A. Anandkumar, D. Foster, D. Hsu, S. Kakade and Y. Liu <a href="http://arxiv.org/abs/1204.6703">A Spectral Algorithm for Latent Dirichlet Allocation</a>, NIPS 2012.
<br>
A. Anandkumar, R. Ge, D. Hsu and S. Kakade. <a href="http://arxiv.org/abs/1302.2684">A Tensor Spectral Approach to Learning Mixed Membership Community Models</a>, COLT 2013.
<br>
</ul>
<br>

 <b> Discussion: </b> When do algorithms rely (too much) on a distributional model?
 <ul>
 <br>
U. Feige and J. Kilian. <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.35.6813">Heuristics for Semirandom Graph Problems</a>, JCSS 2001.
</ul>
<br>

<b> Sparse Recovery and Sparse Coding</b> 
<br>
<ul>
<li>Incoherence and Uncertainty Principles</li>
<li>Orthogonal Matching Pursuit</li>
<li>Compressed Sensing and RIP</li> 
<li>Alternating Minimization via Approximate Gradient Descent <a href="http://people.csail.mit.edu/moitra/docs/Neural.pdf">[slides]</a></li>
<br>
</ul>

<ul>
M. Elad. Sparse and Redundant Representations, Springer 2010.
<br>
B. Olshausen and D. Field. <a href="http://www.nature.com/nature/journal/v381/n6583/abs/381607a0.html">Emergence of Simple-cell Receptive Field Properties by Learning a Sparse Code for Natural Images</a>, Nature 1996.
<br>
D. Spielman, H. Wang and J. Wright. <a href="http://arxiv.org/abs/1206.5882">Exact Recovery of Sparsely-Used Dictionaries</a>, COLT 2012. 
<br>
S. Arora, R. Ge, T. Ma and A. Moitra. <a href="https://arxiv.org/abs/1503.00778">Simple, Efficient and Neural Algorithms for Sparse Coding</a>, COLT 2015
<br>
</ul>
<br>

 <b> Discussion: </b> When does belief propagation (provably) work? 
 <ul>
 <br>
S. Geman and D. Geman. <a href="http://www.stat.cmu.edu/~acthomas/724/Geman.pdf">Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images</a>, Trans. Pattern Analysis and Machine Intelligence 1984. 
</ul>
<br>


<b>Graphical Models, Robustness and Nonconvexity</b> 
<br>
<ul>
<li>Learning Graphical Models</li>
<li>Agnostically Learning a Gaussian</li>
<li>The Landscape of Nonconvex Optimization</li>
<br>
</ul>

<ul>
G. Bresler. <a href="https://arxiv.org/abs/1411.6156">Efficiently Learning Ising Models on Arbitrary Graphs</a>, STOC 2015.
<br>
A. Klivans and R. Meka. <a href="https://arxiv.org/abs/1706.06274">Learning Graphical Models Using Multiplicative Weights</a>, FOCS 2017.
<br>
I. Diakonikolas, G. Kamath, D. Kane, J. Li, A. Moitra and A. Stewart. <a href="https://arxiv.org/abs/1604.06443">Robust Estimators in High-Dimensions with the Computational Intractability</a>, FOCS 2016.
<br>
K. Lai, A. Rao and S. Vempala. <a href="https://arxiv.org/abs/1604.06968">Agnostic Estimation of Mean and Covariance</a>, FOCS 2016.
<br>
M. Charikar, J. Steinhardt and G. Valiant. <a href="https://arxiv.org/abs/1611.02315">Learning from Untrusted Data</a>, STOC 2017.
<br>
R. Ge, C. Jin and Y. Zheng. <a href="https://arxiv.org/abs/1704.00708">No Spurious Local Minima in Nonconvex Low Rank Problems: A Unified Geometric Analysis</a>, ICML 2017.
<br>
</ul>
<br>

 <b> Discussion: </b> Do we have enough average-case assumptions?
 <ul>
 <br>
Q. Berthet and P. Rigollet. <a href="http://arxiv.org/abs/1304.0828">Computational Lower Bounds for Sparse PCA</a>, COLT 2013.
<br>
B. Barak, S. Hopkins, J. Kelner, P. Kothari, A. Moitra and A. Potechin. <a href="https://arxiv.org/abs/1604.03084">A Nearly Tight Sum-of-Squares Lower Bound for the Planted Clique Problem </a>, FOCS 2016.
</ul>
<br>

 <b> Learning Mixture Models</b> 
 <br>
<ul>
<li>Expectation Maximization</li>
<li>Clustering in High-Dimensions</li>
<li>Method of Moments and Systems of Polynomial Equations <a href="http://people.csail.mit.edu/moitra/docs/Dean.pdf">[slides]</a></li>
<br>
</ul>

<ul>
A. Dempster N. Laird and D. Rubin. <a href="http://web.mit.edu/6.435/www/Dempster77.pdf">Maximum Likelihood from Incomplete Data via the EM Algorithm</a>, J. Royal Statistical Society 1977.
<br>
S. Dasgupta. <a href="http://cseweb.ucsd.edu/~dasgupta/papers/mog.pdf">Learning Mixtures of Gaussians</a>, FOCS 1999.
<br>
S. Arora and R. Kannan. <a href="http://projecteuclid.org/euclid.aoap/1106922321">Learning Mixtures of Separated Nonspherical Gaussians</a>, Annals of Applied Probability 2005.
<br>
A. Moitra and G. Valiant. <a href="http://arxiv.org/abs/1004.4223">Settling the Polynomial Learnability of Mixtures of Gaussians</a>, FOCS 2010.
<br>
M. Belkin and K. Sinha. <a href="http://arxiv.org/abs/1004.4864">Polynomial Learning of Distribution Families</a>, FOCS 2010.
<br>
R. Ge, Q. Huang and S. Kakade. <a href="https://arxiv.org/abs/1503.00424">Learning Mixtures of Gaussians in High Dimensions</a>, STOC 2015.
<br>
</ul>
<br>

 <b> Discussion: </b> Is nature an adversary? And if not, how can we model and exploit that?
 <ul>
 <br>
A. Bhaskara, M. Charikar, A. Moitra and A. Vijayaraghavan. <a href="http://arxiv.org/abs/1311.3651">Smoothed Analysis of Tensor Decompositions</a>, STOC 2014.
</ul>
<br>

</ul>


</font></body></html>